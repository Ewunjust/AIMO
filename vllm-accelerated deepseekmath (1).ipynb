{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":73231,"databundleVersionId":8133715,"sourceType":"competition"},{"sourceId":8023365,"sourceType":"datasetVersion","datasetId":4728129},{"sourceId":8077274,"sourceType":"datasetVersion","datasetId":4746046},{"sourceId":8099570,"sourceType":"datasetVersion","datasetId":4782935},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900},{"sourceId":5994,"sourceType":"modelInstanceVersion","modelInstanceId":4761},{"sourceId":11382,"sourceType":"modelInstanceVersion","modelInstanceId":8318},{"sourceId":11394,"sourceType":"modelInstanceVersion","modelInstanceId":8332}],"dockerImageVersionId":30674,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":724.728315,"end_time":"2024-02-29T09:37:08.760349","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-29T09:25:04.032034","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"21267b653022419eb6fc3f47aa4db8ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926e7ccdad6440be85c76931860b744c","placeholder":"​","style":"IPY_MODEL_feef8334edb24f6da22e8bb1d8d80c67","value":"Loading checkpoint shards: 100%"}},"2144e851698b4707ad1c7fc29fe21b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3963993becfa487c9ff725f211915e67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a725e1b0cc4ad78a62beab5f663065","placeholder":"​","style":"IPY_MODEL_fdb32baaed7145d8a8024b615ef242ca","value":" 19/19 [10:48&lt;00:00, 33.24s/it]"}},"5882b6e860be4a0db012a64fc0704a3f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_21267b653022419eb6fc3f47aa4db8ed","IPY_MODEL_d91eb83d016a4381828192a98f798f9b","IPY_MODEL_3963993becfa487c9ff725f211915e67"],"layout":"IPY_MODEL_6a892a5561f742bb9db9f13859c18e90"}},"6a892a5561f742bb9db9f13859c18e90":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926e7ccdad6440be85c76931860b744c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d91eb83d016a4381828192a98f798f9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2144e851698b4707ad1c7fc29fe21b03","max":19,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0693b32889c42b18b9a3844e045d048","value":19}},"e0693b32889c42b18b9a3844e045d048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7a725e1b0cc4ad78a62beab5f663065":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fdb32baaed7145d8a8024b615ef242ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"feef8334edb24f6da22e8bb1d8d80c67":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# credits:\n# https://www.kaggle.com/code/olyatsimboy/aimo-openmath-mistral-baseline\n# https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama\n# https://www.kaggle.com/code/thedrcat/aimo-mixtral-baseline","metadata":{"execution":{"iopub.status.busy":"2024-04-07T10:42:44.072564Z","iopub.execute_input":"2024-04-07T10:42:44.073313Z","iopub.status.idle":"2024-04-07T10:42:44.077990Z","shell.execute_reply.started":"2024-04-07T10:42:44.073282Z","shell.execute_reply":"2024-04-07T10:42:44.077076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**⚠️ Warning: Potential Eligibility Issue for AIMO Competition**\n\nPlease be aware that this notebook may not be eligible for submission to the AIMO competition. It utilizes the DeepSeekMath model and the vLLM framework, some components of which were released after the competition's stipulated deadline of February 23rd. Use of these updated resources could be considered non-compliant with the competition rules, which require all tools and datasets to be publicly available before the deadline. If you are planning to use this notebook or its methods as part of your competition submission, verify the eligibility with the competition organizers to avoid disqualification.\n\n**Introduction to Tree of Thought (ToT) for Enhanced Reasoning in Language Models**\n\nThe Tree of Thought (ToT) is an advanced approach to reasoning with large language models, improving upon traditional methods such as greedy decoding or beam search. While these conventional methods might produce quick solutions, they often fall short in handling complex reasoning tasks effectively. ToT addresses this by exploring a diverse set of reasoning pathways, systematically evaluating them to discern the most accurate and reliable outcomes.\n\n**Incorporation of Process Reward Model (PRM)**\n\nA key component in this enhanced reasoning framework is the integration of a Process Reward Model (PRM). This model acts as a verifier for each reasoning step generated by the language model within the Tree of Thought. The PRM assesses the validity and logical consistency of each step, using predefined criteria to award positive or negative scores based on the appropriateness of the responses. This mechanism ensures that only the most promising reasoning paths are expanded, enhancing the overall quality and reliability of the solutions.\n\n**Fallback to Python-Coding Based Self-Consistency**\n\nIn instances where the PRM does not confirm the validity of the reasoning paths, the framework falls back on a Python-coding based self-consistency approach. This secondary layer involves generating multiple diverse answers through code execution, then aggregating these answers to establish a consensus. This method, detailed in the Self-Consistency in Chains of Thought (SC-CoT) paper, serves as a reliable backup, ensuring that the final answers are not only generated but also validated through practical execution.\n\n**Utilizing DeepSeekMath-7B RL-Tuned Model**\n\nWe employ the DeepSeekMath-7B model, which is fine-tuned with reinforcement learning techniques to enhance its performance on mathematical reasoning tasks. The model's advanced capabilities allow it to produce consistent reasoning paths and significantly reduce arithmetic hallucinations. By integrating the ToT approach with the PRM verification, we harness this model's potential to generate and refine diverse reasoning paths, leading to high accuracy and reliability in solutions.\n\n**Practical Application in This Kernel**\n\nThis kernel will demonstrate how the ToT methodology, supported by PRM verification and Python-coding based self-consistency, can be effectively applied using the DeepSeekMath-7B model. Through practical examples and detailed code implementations, we aim to show a marked improvement in solving complex reasoning problems, particularly in mathematical contexts. This exploration provides valuable theoretical insights and practical tools for researchers and practitioners aiming to leverage advanced AI reasoning strategies in their work.\n","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y torch\n!pip install --no-index --find-links=/kaggle/input/vllm-whl -U vllm","metadata":{"papermill":{"duration":18.075198,"end_time":"2024-02-29T09:25:25.295954","exception":false,"start_time":"2024-02-29T09:25:07.220756","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-13T11:43:23.524683Z","iopub.execute_input":"2024-04-13T11:43:23.525558Z","iopub.status.idle":"2024-04-13T11:46:01.937224Z","shell.execute_reply.started":"2024-04-13T11:43:23.525525Z","shell.execute_reply":"2024-04-13T11:46:01.936279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### vLLM Setup\n\n- **vLLM (LLM Instance)**: The `LLM` class instance is configured to use a model located at `/kaggle/input/deepseek-math`. This setup involves several parameters optimized for performance:\n  - **dtype='half'**: This setting uses 16-bit floating-point numbers (float16) instead of the standard 32-bit. This can significantly reduce memory usage and potentially speed up computations, albeit at the risk of lower numerical precision.\n  - **enforce_eager=True**: Typically, this would enforce eager execution over graph-based execution, making debugging easier and operations more transparent but potentially at a cost to performance.\n  - **gpu_memory_utilization=0.99**: This setting maximizes GPU memory usage to optimize the computation workload, allowing almost the entire GPU memory to be utilized.\n  - **swap_space=4**: Allocates kv cache offload to system RAM, useful in handling larger models or batches that exceed physical GPU memory. May not be used during nrmal operation.\n  - **max_model_len=2048**: Sets the maximum sequence length that the model will process, limiting the scope of inputs to ensure stable performance.\n  - **kv_cache_dtype=\"fp8_e5m2\"**: Utilizes an 8-bit floating-point format for storing key/value pairs in attention mechanisms, trading off precision for speed and memory efficiency. Necessary for generating a sequence of more than 1000 tokens in a single T4.\n  - **tensor_parallel_size=1**: Operations are confined to a single GPU.\n\n### Tokenizer and Model Configuration\n\n- **Tokenization**: The tokenizer associated with the vLLM model is loaded to handle input preprocessing, which is crucial for correctly formatting and encoding the inputs into a form suitable for model processing.\n- **PRM Tokenizer and Model**: A separate tokenizer and model (`AutoModelForCausalLM`) are loaded from a pretrained path. These are specifically used for a Process Reward Model (PRM), designed to evaluate generated responses at each reasoning step based on the presence of specific tokens (good or bad indicators):\n  - **prm_candidate_tokens**: Encodes specific tokens that indicate positive and negative outcomes, aiding in evaluating the output from the reward model.\n  - **step_tag_id**: Represents a specific token used to mark steps or stages in the reasoning process.\n\n### Generation Speed and Determinism\n\n- **Generation Speed**: vLLM is designed to be a high-speed model, potentially sacrificing some deterministic behaviors (like exact reproducibility of results) for faster response generation. This trade-off is particularly evident in environments where quick response generation is prioritized over absolute consistency. Setting a seed might not ensure reproducible outcomes.\n\n### Operational Context\n\n- vLLM is designed to run in it's own Docker environment, which can encapsulate its dependencies and settings. Running in a notebook environment requires some hacky solutions to load and manage models effectively, especially when interfacing with system resources or other software components.","metadata":{}},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\nimport pandas as pd\nfrom tqdm import tqdm\nimport gc\nimport re\nimport sys\nimport subprocess\nfrom collections import defaultdict, Counter\nimport numpy as np\nfrom transformers import (AutoModelForCausalLM,\n    AutoTokenizer,\n    set_seed)\nimport torch\nimport math\n\nllm = LLM(model=\"/kaggle/input/deepseek-math\",\n          dtype='half',\n          enforce_eager=True,\n          gpu_memory_utilization=0.99,\n          swap_space=4,\n          max_model_len=2048,\n          kv_cache_dtype=\"fp8_e5m2\",\n          tensor_parallel_size=1)\n\ntokenizer = llm.get_tokenizer()\n\ngood_token = '+'\nbad_token = '-'\nstep_tag = 'ки'\n\nprm_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/math-shepherd-mistral-7b-prm')\nprm_candidate_tokens = prm_tokenizer.encode(f\"{good_token} {bad_token}\")[1:] # [648, 387]\nstep_tag_id = prm_tokenizer.encode(f\"{step_tag}\")[-1] # 12902\nprm_model = AutoModelForCausalLM.from_pretrained('/kaggle/input/math-shepherd-mistral-7b-prm',\n                                                 torch_dtype=torch.float16,\n                                                 device_map=\"balanced_low_0\").eval()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:51:46.458896Z","iopub.execute_input":"2024-04-13T11:51:46.459296Z","iopub.status.idle":"2024-04-13T11:56:45.556544Z","shell.execute_reply.started":"2024-04-13T11:51:46.459255Z","shell.execute_reply":"2024-04-13T11:56:45.555558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nPRIVATE = True\n\ndf = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/test.csv')\ndf.head()","metadata":{"papermill":{"duration":1.224774,"end_time":"2024-02-29T09:36:31.21757","exception":false,"start_time":"2024-02-29T09:36:29.992796","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-13T11:57:27.880005Z","iopub.execute_input":"2024-04-13T11:57:27.880869Z","iopub.status.idle":"2024-04-13T11:57:27.918466Z","shell.execute_reply.started":"2024-04-13T11:57:27.880840Z","shell.execute_reply":"2024-04-13T11:57:27.917509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(df) < 5:\n    #df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv')\n    PRIVATE = False\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:57:31.243187Z","iopub.execute_input":"2024-04-13T11:57:31.243788Z","iopub.status.idle":"2024-04-13T11:57:31.259681Z","shell.execute_reply.started":"2024-04-13T11:57:31.243757Z","shell.execute_reply":"2024-04-13T11:57:31.258639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def naive_parse(answer):\n    out = []\n    start = False\n    end = False\n    for l in reversed(list(answer)):\n        if l in '0123456789' and not end:\n            start = True\n            out.append(l)\n        else:\n            if start:\n                end = True\n        \n    out = reversed(out)\n    return ''.join(out)","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:57:35.295990Z","iopub.execute_input":"2024-04-13T11:57:35.296468Z","iopub.status.idle":"2024-04-13T11:57:35.302587Z","shell.execute_reply.started":"2024-04-13T11:57:35.296421Z","shell.execute_reply":"2024-04-13T11:57:35.301548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport sys\nimport os\nimport subprocess\n\ndef top_n_strings(input_list, n):\n    # Sort the list based on the prob values in descending order\n    sorted_list = sorted(input_list, key=lambda x: x[1], reverse=True)\n\n    # Get the top n elements\n    top_n = sorted_list[:n]\n\n    # Extract the strings from the top n elements\n    result = [item[0] for item in top_n]\n\n    return result\n\n\ndef process_output(output):\n    result = output\n    \n    try:\n        code = output.split('```')[1][7:]\n\n        with open('code.py', 'w') as fout:\n            fout.write(code)\n\n        batcmd = 'timeout 7 ' + sys.executable + ' code.py'\n        try:\n            shell_output = subprocess.check_output(batcmd, shell=True).decode('utf8')\n            print(shell_output)\n            code_output = round(float(eval(shell_output))) % 1000\n        except:\n            code_output = -1\n            \n        if os.path.exists('code.py'):\n            os.remove('code.py')\n\n        print('CODE RESULTS', code_output)\n    \n    except Exception as e:\n        print(e)\n        print('ERROR PARSING')\n        code_output = -1\n    \n    try:\n        result_output = re.findall(r'\\\\boxed\\{(.*)\\}', result)\n\n        print('BOXED', result_output)\n        if not len(result_output):\n            result_output = naive_parse(result)\n        else:\n            result_output = result_output[-1]\n\n        print('BOXED', result_output)\n        if not len(result_output):\n            result_output = -1\n        \n        else:\n            result_output = round(float(eval(result_output))) % 1000\n    \n    except Exception as e:\n        print(e)\n        print('ERROR PARSING')\n        result_output = -1\n    \n    return result_output, code_output","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:57:37.630337Z","iopub.execute_input":"2024-04-13T11:57:37.631224Z","iopub.status.idle":"2024-04-13T11:57:37.643531Z","shell.execute_reply.started":"2024-04-13T11:57:37.631191Z","shell.execute_reply":"2024-04-13T11:57:37.642519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 1\n\ndef eval_prm(candidates):\n    # Initialize a list to store all the log probabilities\n    all_log_probs = []\n\n    # Process the candidates in batches\n    for i in range(0, len(candidates), batch_size):\n        # Select a batch of candidates\n        batch_candidates = candidates[i:i + batch_size]\n\n        # Encode all candidates into a batch of input IDs\n        encoded_inputs = [prm_tokenizer.encode(candidate, return_tensors=\"pt\") for candidate in batch_candidates]\n\n        # Pad the encoded inputs to the same length\n        max_length = max([input_id.shape[1] for input_id in encoded_inputs])  # Find the longest sequence\n        padded_inputs = [\n            torch.nn.functional.pad(input_id, (0, max_length - input_id.size(1)), value=prm_tokenizer.pad_token_id) for\n            input_id in encoded_inputs]\n        input_ids = torch.cat(padded_inputs, dim=0).to(\"cuda:1\")  # Concatenate the padded inputs into a tensor\n\n        with torch.no_grad():\n            logits = prm_model(input_ids).logits[:, :, prm_candidate_tokens]\n\n            scores = logits.softmax(dim=-1)[:, :, 0].squeeze()\n\n            # Extract log probabilities for the specific candidate tokens\n            log_probs = scores.log()\n\n            if batch_size == 1:\n                batch_log_probs = log_probs[-1].flatten()\n            else:\n                batch_log_probs = log_probs[:, -1].flatten()\n\n            # Collect the log probabilities from this batch\n            all_log_probs.extend(batch_log_probs.cpu().tolist())\n\n    return all_log_probs","metadata":{"execution":{"iopub.status.busy":"2024-04-13T11:57:43.669585Z","iopub.execute_input":"2024-04-13T11:57:43.670350Z","iopub.status.idle":"2024-04-13T11:57:43.680009Z","shell.execute_reply.started":"2024-04-13T11:57:43.670317Z","shell.execute_reply":"2024-04-13T11:57:43.679135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\nstop_words.append(\"\\n\")\n\ntool_stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']\ntool_stop_words.append(\"```output\")\n\nsampling_params = SamplingParams(temperature=1.0,\n                                 max_tokens=256,\n                                 stop=stop_words)\n\ntool_sampling_params = SamplingParams(temperature=1.0,\n                                      max_tokens=2048,\n                                      stop=tool_stop_words)\n\ncot_instruction = \"\\nPlease reason step by step, and put your final answer within \\\\boxed{}.\"\n\ntool_instruction = '\\nPlease integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\\\boxed{}.'\nn = 1\nall_prompts = []\ntotal_results = []\ntotal_answers = []\n\nfor i in tqdm(range(len(df))):\n    id_ = df['id'].loc[i]\n    problem = df['problem'].loc[i]\n\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": problem + cot_instruction\n        }\n    ]\n\n    base_prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False\n    )\n    current_level = 0\n\n    current_level_nodes = [(base_prompt, 0)]  # Tuple of (node, cumulative_logprob)\n    completed_paths = []\n\n    while len(completed_paths) < n:\n        # Prepare batch for generation\n        batch_prompts = [node for node, _ in current_level_nodes]\n        batch_responses = llm.generate(batch_prompts*16, sampling_params)  # Generate for all nodes in a batch\n\n        prm_inputs = []  # To collect all candidates for reward model evaluation\n        mapping = []  # To map back reward model scores to corresponding nodes\n\n        # Collect candidates for reward model evaluation\n        for parent_node, parent_cum_logprob in current_level_nodes:\n            for candidate in batch_responses:\n                if parent_node + candidate.outputs[0].text + \"\\n\" not in [prm_input[1] for prm_input in prm_inputs]:\n                    new_node = parent_node + candidate.outputs[0].text + \"\\n\"\n                    cumulative_tokens = len(candidate.prompt_token_ids) + len(candidate.outputs[0].token_ids)\n                    prm_inputs.append((new_node[:-1] + \" \" + step_tag, new_node, parent_cum_logprob, cumulative_tokens))\n                    mapping.append(len(prm_inputs) - 1)  # Store the index for mapping back the score\n\n        # Batch reward model evaluation\n        prm_scores = eval_prm([prm_input for prm_input, _, _, _ in prm_inputs])\n        next_level_nodes = []\n\n        # Distribute candidates back to their parent nodes\n        for idx, (_, node, parent_cum_logprob, cumulative_tokens) in enumerate(prm_inputs):\n            score = prm_scores[idx]  # Get the corresponding score\n            new_cum_logprob = parent_cum_logprob + score  # Update cumulative log probability\n\n            # Check for completions and sufficient score\n            if \"```\" in node or \"\\\\boxed\" in node.split(\"\\n\\n\")[-1]:\n                completed_paths.append((node, new_cum_logprob))\n            elif score > math.log(0.8) and cumulative_tokens <= 2000:  # Threshold check\n                next_level_nodes.append((node, new_cum_logprob))\n\n        # Prune to keep only the top 'n' candidates based on cumulative log probability\n        next_level_nodes.sort(key=lambda x: x[1], reverse=True)  # Sort nodes by their cumulative log probability\n        current_level_nodes = next_level_nodes[:n]  # Keep only the top 'n' nodes\n        current_level += 1\n\n        # If we already have 'n' completed paths, no need to continue\n        if len(completed_paths) >= n:\n            break\n        if not current_level_nodes or current_level > 20:\n            if not completed_paths:\n                messages = [\n                    {\n                        \"role\": \"user\",\n                        \"content\": problem + tool_instruction\n                    }\n                ]\n\n                base_prompt = tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False\n                ) + \"```python\\n\"\n\n                raw_outputs = llm.generate([base_prompt]*8, tool_sampling_params)\n\n                for response in raw_outputs:\n                    completed_paths.append(\"```python\\n\" + response.outputs[0].text)\n            break\n\n    results = []\n    answers = []\n\n    for path in completed_paths:\n        try:\n            if \"```python\\n\" in path:\n                result_output, code_output = process_output(path)\n                gc.collect()\n                \n                results.append(code_output)\n                answers.append(code_output)\n            else:\n                raw_output = path[0].split(\"\\n\\n\")[-1]\n                result_output, code_output = process_output(raw_output)\n                gc.collect()\n\n                results.append(result_output)\n                answers.append(result_output)\n\n        except Exception as e:\n            print(e)\n            result_output, code_output = -1, -1\n            logprob = -10000\n\n            results.append(result_output)\n            answers.append(result_output)\n\n    total_results.append(results)\n    total_answers.append(answers)","metadata":{"papermill":{"duration":34.259365,"end_time":"2024-02-29T09:37:05.548829","exception":false,"start_time":"2024-02-29T09:36:31.289464","status":"completed"},"tags":[],"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-04-13T11:57:47.179885Z","iopub.execute_input":"2024-04-13T11:57:47.180254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\n\nfinal_answers = []\n\nfor a, b in zip(total_answers, total_results):\n    a = np.array(a)\n    b = np.array(b)\n    a[a < 0] = b[a < 0]\n\n    pred = Counter(a.tolist()).most_common(2)\n\n    try:\n        ans = pred[0][0] if not pred[0][0] < 0 else pred[1][0]\n    except:\n        if len(a) == 1:\n            ans = a[0]\n        else:\n            ans = 0\n\n    final_answers.append(ans)\n    print(ans)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T14:13:01.750064Z","iopub.execute_input":"2024-04-12T14:13:01.751230Z","iopub.status.idle":"2024-04-12T14:13:01.763049Z","shell.execute_reply.started":"2024-04-12T14:13:01.751195Z","shell.execute_reply":"2024-04-12T14:13:01.762088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['answer'] = final_answers","metadata":{"execution":{"iopub.status.busy":"2024-04-12T14:13:04.465061Z","iopub.execute_input":"2024-04-12T14:13:04.465849Z","iopub.status.idle":"2024-04-12T14:13:04.472344Z","shell.execute_reply.started":"2024-04-12T14:13:04.465816Z","shell.execute_reply":"2024-04-12T14:13:04.471355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-12T14:13:06.178900Z","iopub.execute_input":"2024-04-12T14:13:06.179528Z","iopub.status.idle":"2024-04-12T14:13:06.189287Z","shell.execute_reply.started":"2024-04-12T14:13:06.179499Z","shell.execute_reply":"2024-04-12T14:13:06.188260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['id','answer']].to_csv(\"submission.csv\", header=True, index=False)","metadata":{"papermill":{"duration":0.021128,"end_time":"2024-02-29T09:37:05.574782","exception":false,"start_time":"2024-02-29T09:37:05.553654","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-12T14:13:09.810019Z","iopub.execute_input":"2024-04-12T14:13:09.810373Z","iopub.status.idle":"2024-04-12T14:13:09.834210Z","shell.execute_reply.started":"2024-04-12T14:13:09.810346Z","shell.execute_reply":"2024-04-12T14:13:09.833240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['id','answer']].head()","metadata":{"papermill":{"duration":0.014339,"end_time":"2024-02-29T09:37:05.594605","exception":false,"start_time":"2024-02-29T09:37:05.580266","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-12T14:13:12.237421Z","iopub.execute_input":"2024-04-12T14:13:12.238056Z","iopub.status.idle":"2024-04-12T14:13:12.247868Z","shell.execute_reply.started":"2024-04-12T14:13:12.238024Z","shell.execute_reply":"2024-04-12T14:13:12.246991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Concluding Observations and Recommendations**\n\nThroughout our exploration with the DeepSeekMath-7B RL-tuned model and the application of the Tree of Thought (ToT) approach, pivotal insights have emerged, shaping my understanding of the model's capabilities and its limitations:\n\n1. **Performance Upper Bound**: Analysis of the model's performance on the MATH test dataset has revealed an apparent \"upper bound\" to its capabilities. This limitation suggests that even with optimal tuning and application of sophisticated reasoning frameworks like ToT, there may be a hard limit to the complexity of problems the model can handle effectively. Interestingly, this upper bound might also explain the high scores observed in other notebooks, which could be more a factor of fortunate seeds rather than superior model performance or methodology.\n\n2. **Challenges with Prompt Engineering and Reliability**: The model has shown limited responsiveness to nuanced prompt engineering, restricting its utility in settings where fine-tuning through prompts is essential. Moreover, despite the theoretical promise of ToT in enhancing reliability through multiple reasoning pathways, it has not consistently elevated the model's performance to overcome complex challenges within the dataset.\n\n3. **Recommendations for Further Development**:\n   - **In-depth Analysis and Enhanced Training**: Deeper analysis into the model’s inherent limitations and targeted training may help in pushing its performance beyond the observed upper bound.\n   - **Refinement of ToT**: Improving the ToT methodology by incorporating more robust evaluation metrics or additional supportive mechanisms could bolster its effectiveness.\n\n**Future Directions**\n\nConsidering these insights, future work should aim at identifying and transcending the performance limitations of the current model. Efforts should focus on expanding the model's problem-solving capabilities through advanced training, refining reasoning methodologies, and potentially exploring new model architectures or hybrid approaches. The objective remains not only to surpass the current leaderboard scores but to expand the boundaries of AI capabilities in solving highly complex problems.","metadata":{}}]}